# ðŸš€ AutoTuning Framework Documentation

This system is a **modular, automated, and extensible fine-tuning pipeline** for Hugging Face models. It supports instruction tuning, code generation, chat fine-tuning, question answering, RAG, quantization, PEFT with LoRA, evaluation, and model export in various formats (Torch, TensorFlow, GGUF).

---

## ðŸ”§ 1. Configuration Module (`GlobalConfig.py`)

### Class: `global_config`

Initializes and validates user configuration.

**Key Parameters:**

* `ModelName`: Pretrained model name (e.g., `gpt2`)
* `FineTuningType`: Type of fine-tuning (`instruction_fine_tuning`, `code_generation`, `chat_fine_tuning`, `question_answering`, `rag_fine_tuning`)
* `DatasetPath`: Path to the dataset or HuggingFace dataset ID
* `ModelSeqMaxLength`: Sequence truncation length
* `QuantizationType4Bit8Bit`: Supports `4bit` or `8bit`
* `PeftType`: Type of parameter-efficient tuning (`LORA`, `QLORA`)
* `SaveFormat`: Final model format (`torch`, `tensorflow`, `gguf`)
* `ModelDir`: Output directory for model
* `EvalSaveFormat`: Format for saving eval results (`json`, `csv`)
* `FSDP`: Enable FSDP for multi-GPU training
* `HfToken`: Required Hugging Face token

**Utility Methods:**

* `GetPeftConfig()`
* `GetTrainingArguments()`
* `GetFSDP()`
* `get_full_config()`

---

## ðŸ“¦ 2. Dataset Loading (`DatasetUpLoading.py`)

### Class: `UploadDataset`

Supports dynamic loading and preprocessing for multiple fine-tuning types:

* `instruction_fine_tuning`: Expects fields `instruction`, `input`, `output`
* `code_generation`: Expects `prompt`, `completion`
* `chat_fine_tuning`: Expects `messages` with roles and values
* `question_answering`, `rag_fine_tuning`: Expects `context`, `question`, `answer`

**Built-in datasets (if path not provided):**

* Alpaca
* Glaive Code Assistant
* ShareGPT
* MS MARCO
* Mou3az QA Choices

All modes include mapping functions to tokenize text and create labels based on `[sep]` token.

---

## ðŸ§  3. Model Tuning (`ModelingAndTuning.py`)

### Class: `ModelLoadingAndTuning`

Main training orchestration logic.

**Steps:**

1. Load dataset using `UploadDataset`
2. Load tokenizer and model
3. Apply quantization (4bit or 8bit)
4. Add PEFT using LoRA (`get_peft_model`)
5. Configure training args (`Trainer`)
6. Add evaluation callback (accuracy, f1, perplexity)
7. Train with `trainer.train()`
8. Merge LoRA layers back into base model
9. Save in specified format using `ConvertModel`
10. Save evaluation results to `.json` or `.csv`

---

## ðŸ’¾ 4. Model Conversion (`GetModel.py`)

### Class: `ConvertModel`

Exports trained model to chosen format:

* **`torch`**: Saves using `save_pretrained`
* **`tensorflow`**: Converts from PyTorch using `from_pt=True`
* **`gguf`**:

  * Installs dependencies
  * Uses `llama.cpp/convert_hf_to_gguf.py`
  * Output saved as `output_gguf.gguf`

---

## ðŸ§ª 5. Execution Runner (`runner.py`)

Entry point for training pipeline.

**Usage:**

```bash
cd ./GsocProject/autotuning/
python -m backend.runner
```

**Runner Flow:**

* Instantiate `global_config`
* Generate config using `get_full_config`
* Create `ModelLoadingAndTuning` instance
* Call `LoadItTrainIt()` to run training + evaluation + export

---

## âœ… Supported Features

| Feature                | Support |
| ---------------------- | ------- |
| HF Model Loading       | âœ… Yes   |
| Instruction Tuning     | âœ… Yes   |
| Chat Tuning            | âœ… Yes   |
| Code Generation        | âœ… Yes   |
| QA & RAG               | âœ… Yes   |
| PEFT (LoRA/QLORA)      | âœ… Yes   |
| Quantization (4/8bit)  | âœ… Yes   |
| FSDP (Multi-GPU)       | âœ… Yes   |
| Export: Torch          | âœ… Yes   |
| Export: TensorFlow     | âœ… Yes   |
| Export: GGUF           | âœ… Yes   |
| Evaluation: Accuracy   | âœ… Yes   |
| Evaluation: F1         | âœ… Yes   |
| Evaluation: Perplexity | âœ… Yes   |

---

## ðŸ“Œ Final Notes

* Make sure you supply a **valid Hugging Face token**
* Compatible with **Colab**, **multi-GPU**, **32-bit machines**, and **CLI-based environments**
  
For contributions, feature requests, or issues, open a ticket at `https://github.com/yourrepo/autotuning`
